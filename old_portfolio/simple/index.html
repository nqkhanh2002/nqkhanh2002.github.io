
<html>
    <head>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
        <title>Nguyen Quoc Khanh | BON</title>
        <meta content="Nguyen Quoc Khanh, nqkhanh2002.github.io" name="keywords" />
        <style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration:none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration:none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight:bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em, i {
            font-style:italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.edu {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 0.5em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.edu div {
            padding-left: 100px;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }
        img.icon {
            margin-bottom: 0.5em;
            float: left;
            width: 75px;
            height: 75px;
        }

        span.blurb {
            font-style:italic;
            display:block;
            margin-top:0.75em;
            margin-bottom:0.5em;
        }

        pre, code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');



</script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-66888300-1', 'auto');
    ga('send', 'pageview');

</script>

<body>
    <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
        <div style="margin: 0px auto; width: 100%;">
            <img title="NguyenQuocKhanh" style="float: left; padding-left: .01em; height: 140px;" src="NguyenQuocKhanh.jpg" />
            <div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Nguyen Quoc Khanh (Kane)</span><br />
                <div itemscope itemtype="https://schema.org/Person"><a itemprop="sameAs" content="https://orcid.org/0009-0007-4278-149X" href="https://orcid.org/0009-0007-4278-149X" target="orcid.widget" rel="noopener noreferrer" style="vertical-align:top;"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" alt="ORCID iD icon">orcid.org/0009-0007-4278-149X</a></div>
                <br>
                Email: nqkdeveloper[AT]gmail[dot]com
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2>About Me (<a href='https://github.com/nqkhanh2002'>GitHub</a>) (<a href='https://scholar.google.com/citations?user=lqq4oN4AAAAJ&hl=en'>Google scholar</a>) (<a href='https://www.linkedin.com/in/nqkdeveloperai/'>LinkedIn</a>)</h2>
            <div class="paper">
                Nguyen Quoc Khanh is currently working as a Data Scientist at <a href='https://www.linkedin.com/company/thinkprompt'>ThinkPrompt</a> and an AI Engineer intern at <a href='https://vinbigdata.com/en'>VinBigData</a>, VinGroup. He received his 
                B.E. degree from <a href='https://en.uit.edu.vn/'>University of Information Technology - UIT</a> in 2024. Before that, he joined ThinkPrompt (2023) and VinBigData (2024).            
            </br>
                About his research preference: He is interested in curiosity-driven research on unsolved problems. He is keen on taking challenges of new problems as long as they are valuable in research or business.
            </br>
                <font color="red"><strong>I am open for a new position! Please contact me via email.</strong></font>

                <!-- Besides, he was the major contributor of the repo <a href="https://github.com/open-mmlab/mmselfsup">MMSelfsup</a> in OpenMMLab project. -->
                
                <!-- My research interests include unsupervised learning and computer vision from 2016, and computer graphics, character animation, 3D content generation from 2020. -->
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2 id="edu">Education</h2>
            <!-- <div class="edu" id="cuhk"><img class="icon" src="icons/cuhk_icon.png" title="cuhk" />
                <div> <font size="20"><strong>The Chinese University of Hong Kong (MMLab)</strong></font><br />
                    <br>
                    <font color="gray">August 2017 - August 2020 (1 year shortened). Ph.D. in Information Engineering.</font>
                    <br>
                    <font color="gray">August 2016 - August 2017. Research assistant.</font>
                    <br>
                    <font color="gray">With “Top 100 Chinese Rising Stars in Artificial Intelligence”.</font>
                </div>
                <div class="spanner"></div>
            </div> -->
            <div class="edu" id="thu"><img class="icon" src="icons/uit_icon.png" title="thu" />
                <div> <font size="20"><strong>University of Information Technology - UIT</strong></font><br />
                    <br>
                    <font color="gray">August 2020 - December 2024. Bachelor of Information System.</font>
                    <br>
                    <font color="gray">With Very Good Graduation.</font>
                </div>
                <div class="spanner"></div>
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2 id="work">Professional Experience</h2>
            <div class="edu" id="vinbigdata"><img class="icon" src="icons/vinbigdata_icon.png" title="tencent" />
                <div> <font size="20"><strong>Vingroup AI Engineer Training Program, VinBigData, VinGroup</strong></font><br />
                    <br>
                    <font color="gray">July 2024 - Now. <br />
                        AI Engineer<br />
                        Training in cores subjects of AI: Math, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing, MLOPs, Data Engineering, AI Ethics.<br />
                    </font>
                </div>
                <div class="spanner"></div>
            </div>

            <div class="edu" id="thinkprompt"><img class="icon" src="icons/thinkprompt_icon.jpg" title="huawei" />
                <div> <font size="20"><strong>THINKPROMPT CO., LTD</strong></font><br />
                    <br>
                    <font color="gray">17 July 2023 - Now. <br />
                        Junior Data Scientist, Lead Project<br />
                        Research in Machine Learning Modeling, MLOPs, Data Engineering.<br />
                    </font>
                    <br>
                </div>
                <div class="spanner"></div>
            </div>

            <!-- <div class="edu" id="sensetime"><img class="icon" src="icons/sensetime_icon.png" title="sensetime" />
                <div> <font size="20"><strong>SenseTime</strong></font><br />
                    <br>
                    <font color="gray">Dec 2017 - April 2018, Mar 2016 - Oct 2016. <br />
                        Research Intern.<br />
                        Research on large-scale clustering and semi-supervised learning for face recognition. <br />
                        Research on lane detection for autonomous driving. 
                    </font>
                    <br>
                </div>
                <div class="spanner"></div>
            </div> -->

            <!-- <div class="edu" id="cuhk"><img class="icon" src="icons/cuhk_icon.png" title="cuhk" />
                <div> <font size="20"><strong>Multimedia Laboratory, The Chinese University of Hong Kong</strong></font><br />
                    <br>
                    <font color="gray">Oct 2016 - Aug 2017, July 2015 - Sept 2015. <br />
                        Junior Research Assistant.<br />
                        Research on self-supervised learning and optical flow estimation.
                    </font>
                    <br>
                </div>
                <div class="spanner"></div>
            </div> -->

        </div>
    </div>




<!-- 
    <div style="clear: both;">
        <div class="section">
            <h2>News</h2>
            <div class="paper">
                <ul>
                    [03/07/2020] One paper was accepted to ECCV 2020 as oral.<br>
                    [16/06/2020] Self-Supervised Learning toolbox and benchmark codebase <a href='https://github.com/open-mmlab/OpenSelfSup'>OpenSelfSup</a> is released.<br>
                    [13/03/2020] Four papers were accepted to CVPR 2020, two of them were selected as oral presentations.<br>
                    [01/11/2019] Our team won the 1st places in all 4 tracks on Facebook AI Self-Supervision Challenge.<br>
                    [23/07/2019] One paper was accepted to ICCV 2019.<br>
                    [24/02/2019] Three papers were accepted to CVPR 2019.
                </ul>
            </div>
        </div>
    </div>
 -->

    <div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Productized Projects</h2>

        <div class="paper" id="arxiv"><img class="paper" src="images/project_1.jpg" title="tencent-s2f" />
            <div> <strong>Computer Vision-Based Advanced Driver Assistance System (ADAS) Developmen</strong><br />
                <br>
                Achievements: real-time high implement computer vision-based ADAS system on Object Detection, Multi-Object Tracking, Lane Detection, Lane Departure Warning System, 
                Lane departure warning system, Forward Collision Warning System.<br />
                Productization: 
                <ul>
                    <li>Developed ADAS functionalities, including FCWS, LDWS, and LKAS, using only visual sensors.</li>
                    <li>Integrated object detection using YOLO models (YOLOv5 to YOLOv10, EfficientDet) and lane detection with Ultra-Fast-Lane-Detection-v2 in ONNX/TensorRT.</li>
                    <li>Enhanced tracking capabilities with ByteTrack for multi-object tracking and trajectory prediction.</li>
                    <li>Provided example scripts for lane and object detection using ONNX/TensorRT models for ease of deployment.</li>
                    <li>Implemented model quantization techniques (e.g., float16) for optimized model size and performance.</li>
                    <li>Developed a fully operational video inference pipeline with configurable lane and object detection models.</li>
                    <li>Implemented a traditional image processing pipeline for lane detection using camera calibration, distortion correction, perspective transforms, and color-based thresholding techniques.</li>
                    <li>Created a robust lane pixel detection algorithm that includes lane curvature calculation and vehicle position detection relative to lane center.</li>
                    <li>Compared the performance of traditional image processing techniques with deep learning models for lane detection and tracking performance.</li>
                </ul>
                Publicity: <a href="https://github.com/nqkhanh2002/ADAS-LDWS-LKAS-FCWS">Code</a>, <a href="https://www.youtube.com/watch?v=j-Rbf1Wvl6M&list=PLlfxjRXHE-s3eR29Ah2lBhH0QqGxk97iM">Demo</a>
            </div>
            <div class="spanner"></div>
        </div>

        <!-- * Projects in Tencent are undisclosed. -->
        <!-- <div class="paper" id="arxiv"><img class="paper" src="images/huawei-rigging.png" title="huawei-rigging" />
            <div> <strong>Data-Efficient Auto-Rigging Solution for Biped Characters @Huawei 2021-2022</strong><br />
                <br>
                Achievements: robust results for variance in shape, pose and mesh topology, industry-leading accuracy and efficiency.<br />
                Related products: <a href="https://developer.huawei.com/consumer/cn/doc/graphics-Guides/auto-rigging-0000001272237232">HMS Core</a>.<br />
                Publicity: <a href="https://developer.huawei.com/consumer/en/training/course/live/C101667029313149892">Huawei Developer Conference 2022 (start from 1:01:00)</a>,
                           <a href="https://www.bilibili.com/video/BV1vP411L7ga/?vd_source=c0d0c6d3dd06dd38049f3b95ada3e3d3">Bilibili</a>. <br />
            </div>
            <div class="spanner"></div>
        </div> -->

        <!-- <div class="paper" id="arxiv"><img class="paper" src="images/mmselfsup.png" title="mmselfsup" />
            <div> <strong>MMSelfSup Open-source Project @MMLab 2020-2021</strong><br />
                <br>
                Achievements: MMSelfSup is a widely-used open-source self-supervised representation learning toolbox based on PyTorch. Github stars: ~3.1k.<br />
                Project page: <a href="https://github.com/open-mmlab/mmselfsup">mmselfsup</a><br />
                Related products: OpenMMLab platform
            </div>
            <div class="spanner"></div>
        </div> -->

        </div>
    </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Selective Publications</h2>

                <div class="paper" id="arxiv"><img class="paper" src="images/ventus.jpg" title="ventus" />
                    <div> <strong>Optimal Model of Horse Racing Competition Decision
                        Management Based on Machine Learning <font color="orange"><strong>(Updating)</strong></font></strong><br />
                        <br>
                        <strong>Nguyen Quoc Khanh</strong>, Nguyen Phuoc Sang, Vu Anh Tran
                        
                        <br>
                        <em>White Paper, ThinkPrompt CO., LTD</em><br />
                        <br>
                        <a href=''>[Paper]</a>
                        <!-- <a href='https://hanchaoliu.github.io/Prog-MoGen/'>[Project Page]</a>
                        <a href='https://github.com/HanchaoLiu/ProgMoGen'>[Code]</a>
                        <a href='https://www.youtube.com/watch?v=Y59EGigPiL8'>[Demo]</a> -->
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="arxiv"><img class="paper" src="images/industry_ppe.jpg" title="ventus" />
                    <div> <strong>Human Safety and Personal Protective Equipment Detection in Manufacturing Industry using Real-Time Deep Learning methods
                        <br>
                        <strong>Nguyen Quoc Khanh</strong>
                        <br>
                        <!-- <em>White Paper, ThinkPrompt CO., LTD</em><br /> -->
                        <br>
                        <a href=''>[Paper]</a>
                        <!-- <a href='https://hanchaoliu.github.io/Prog-MoGen/'>[Project Page]</a>
                        <a href='https://github.com/HanchaoLiu/ProgMoGen'>[Code]</a>
                        <a href='https://www.youtube.com/watch?v=Y59EGigPiL8'>[Demo]</a> -->
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="arxiv"><img class="paper" src="images/medical_ppe.jpg" title="ventus" />
                    <div> <strong>Medical Personal Protective Equipment Monitoring with Real-Time Deep Learning Techniques
                        <br>
                        <strong>Nguyen Quoc Khanh</strong>
                        
                        <br>
                        <!-- <em>White Paper, ThinkPrompt CO., LTD</em><br /> -->
                        <br>
                        <a href=''>[Paper]</a>
                        <!-- <a href='https://hanchaoliu.github.io/Prog-MoGen/'>[Project Page]</a>
                        <a href='https://github.com/HanchaoLiu/ProgMoGen'>[Code]</a>
                        <a href='https://www.youtube.com/watch?v=Y59EGigPiL8'>[Demo]</a> -->
                    </div>
                    <div class="spanner"></div>
                </div>


                <!-- <div class="paper" id="arxiv"><img class="paper" src="images/cvpr24-humangaussian.png" title="humangaussian" />
                    <div> <strong>HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting</strong><br />
                        <br>
                        Xian Liu, <strong>Xiaohang Zhan</strong>, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu
                        <br>
                        <em>CVPR 2024 (<font color="orange"><strong>Highlight</strong></font>, 11.9% of accepted papers)</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2311.17061'>[Paper]</a>
                        <a href='https://alvinliu0.github.io/projects/HumanGaussian'>[Project Page]</a>
                        <a href='https://github.com/alvinliu0/HumanGaussian'>[Code]</a>
                        <a href='https://www.youtube.com/watch?v=S3djzHoqPKY'>[Demo]</a>
                    </div>
                    <div class="spanner"></div>
                </div> -->
<!-- 
                <div class="paper" id="pami"><img class="paper" src="images/eccv20-dgp.png" title="dgp" />
                    <div> <strong>Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation</strong><br />
                        <br>
                        Xingang Pan, <strong>Xiaohang Zhan</strong>, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo
                        <br>
                        <em>TPAMI 2021 & ECCV 2020 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href='https://ieeexplore.ieee.org/document/9547753'>[Paper]</a>
                        <a href='https://xingangpan.github.io/projects/DGP.html'>[Project Page]</a>
                        <a href="https://github.com/XingangPan/deep-generative-prior">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR20"><img class="paper" src="images/cvpr20-deocc.png" title="cvpr20-deocc" />
                    <div> <strong>Self-Supervised Scene De-occlusion</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Lin, Chen Change Loy
                        <br>
                        <em>CVPR 2020 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Self-Supervised_Scene_De-Occlusion_CVPR_2020_paper.pdf'>[Paper]</a>
                        <a href='projects/deocclusion/'>[Project Page]</a>
                        <a href="https://github.com/XiaohangZhan/deocclusion">[Code]</a>
                        <a href='https://www.youtube.com/watch?v=xIHCyyaB5gU'>[Demo]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR20"><img class="paper" src="images/cvpr20-odc.png" title="cvpr20-odc" />
                    <div> <strong>Online Deep Clustering for Unsupervised Representation Learning</strong><br />
                        <br>
                        <strong>Xiaohang Zhan*</strong>, Jiahao Xie*, Ziwei Liu, Yew-Soon Ong, Chen Change Loy
                        <br>
                        <em>CVPR 2020</em><br />
                        <br>
                        <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Online_Deep_Clustering_for_Unsupervised_Representation_Learning_CVPR_2020_paper.pdf'>[Paper]</a>
                        <a href="https://github.com/open-mmlab/OpenSelfSup">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR2019"><img class="paper" src="images/cvpr19-gcn.png" title="cvpr19-gcn" />
                    <div style="padding-top: 2%"> <strong>Learning to Cluster Faces on an Affinity Graph</strong><br />
                        <br>
                        Lei Yang, <strong>Xiaohang Zhan</strong>, Dapeng Chen, Junjie Yan, Chen Change Loy, Dahua Lin
                        <br>
                        <em>CVPR 2019 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href='http://personal.ie.cuhk.edu.hk/~ccloy/files/cvpr_2019_cluster.pdf'>[PDF]</a>
                        <a href='https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Learning_to_Cluster_Faces_on_an_Affinity_Graph_CVPR_2019_paper.pdf'>[Paper]</a>
                        <a href='https://github.com/yl-1993/learn-to-cluster'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR2019"><img class="paper" src="images/cvpr19-longtail.png" title="cvpr19-longtail" />
                    <div> <strong>Large-scale Long-Tailed Recognition in an Open World</strong><br />
                        <br>
                        Ziwei Liu*, Zhongqi Miao*, <strong>Xiaohang Zhan</strong>, Jiayun Wang, Boqing Gong, Stella X. Yu
                        <br>
                        <em>CVPR 2019 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Large-Scale_Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.pdf">[Paper]</a>
                        <a href='https://liuziwei7.github.io/projects/LongTail.html'>[Project Page]</a>
                        <a href='https://github.com/zhmiao/OpenLongTailRecognition-OLTR'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR2019"><img class="paper" src="images/cvpr19-cmp.png" title="cvpr19-cmp" />
                    <div> <strong>Self-Supervised Learning via Conditional Motion Propagation</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Xingang Pan, Ziwei Liu, Dahua Lin, Chen Change Loy
                        <br>
                        <em>CVPR 2019 </em><br />
                        <br>
                        <a href='https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhan_Self-Supervised_Learning_via_Conditional_Motion_Propagation_CVPR_2019_paper.pdf'>[Paper]</a>
                        <a href='http://mmlab.ie.cuhk.edu.hk/projects/CMP/'>[Project Page]</a>
                        <a href='https://github.com/XiaohangZhan/conditional-motion-propagation/'>[Code]</a>
                        <a href='https://www.youtube.com/watch?v=6R_oJCq5qMw'>[Demo]</a>
                    </div>
                    <div class="spanner"></div>
                </div>
            
                <div class="paper" id="ECCV2018"><img class="paper" src="images/eccv18-cdp.png" title="eccv18-cdp" />
                    <div> <strong>Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Ziwei Liu, Junjie Yan, Dahua Lin, Chen Change Loy
                        <br>
                        <em>ECCV 2018</em><br />
                        <br>
                        <a href='https://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaohang_Zhan_Consensus-Driven_Propagation_in_ECCV_2018_paper.pdf'>[Paper]</a>
                        <a href='http://mmlab.ie.cuhk.edu.hk/projects/CDP/'>[Project Page]</a>
                        <a href='https://github.com/XiaohangZhan/cdp/'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div> -->

            <!-- <h2 id="confpapers">Other Publications</h2>

                <!-- <div> <strong>2024</strong><br /> -->
<!--                 
                    <div class="paper" id="arxiv"><img class="paper" src="images/pg24-vdm.png" title="vdm" />
                        <div> <strong>DreamMapping: High-Fidelity Text-to-3D Generation via Variational Distribution Mapping</strong><br />
                            <br>
                            Zeyu Cai, Duotun Wang, Yixun Liang, Zhijing Shao, Ying-Cong Chen, <strong>Xiaohang Zhan</strong>, Zeyu Wang
                            <br>
                            <em>Pacific Graphics 2024<br />
                            <br>
                            <a href='https://www.arxiv.org/abs/2409.05099'>[Paper]</a>
                        </div>
                        <div class="spanner"></div>
                    </div>
                

                <div class="paper" id="arxiv"><img class="paper" src="images/arxiv-headevolver-s.png" title="headevolver" />
                    <div> <strong>HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation</strong><br />
                        <br>
                        Duotun Wang*, Hengyu Meng*, Zeyu Cai*, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Ying Shan, <strong>Xiaohang Zhan</strong>, Zeyu Wang
                        <br>
                        <em>arXiv preprint</em><br />
                        <br>
                        <a href="https://arxiv.org/abs/2403.09326">[Paper]</a>
                        <a href='https://www.duotun-wang.co.uk/HeadEvolver/'>[Project Page]</a>
                        
                        <br>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="arxiv"><img class="paper" src="images/tapmo-arxiv.png" title="tapmo" />
                    <div> <strong>TapMo: Shape-aware Motion Generation of Skeleton-free Characters</strong><br />
                        <br>
                        Jiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, <strong>Xiaohang Zhan</strong>, Gang Yu, Ying Shan
                        <br>
                        <em>ICLR 2024</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2310.12678'>[Paper]</a>
                    </div>
                    <div class="spanner"></div>
                </div> -->

            <!-- <div> <strong>2023</strong><br /> -->
                <!-- <div class="paper" id="arxiv"><img class="paper" src="images/semanticboost-arxiv.png" title="semanticboost" />
                    <div> <strong>SemanticBoost: Elevating Motion Generation with Augmented Textual Cues</strong><br />
                        <br>
                        Xin He, Shaoli Huang, <strong>Xiaohang Zhan</strong>, Chao Weng, Ying Shan
                        <br>
                        <em>arXiv preprint</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2310.20323'>[Paper]</a>
                        <a href='https://blackgold3.github.io/SemanticBoost/'>[Project Page]</a>
                        <a href='https://github.com/blackgold3/SemanticBoost'>[Code]</a>
                        <a href='https://discord.gg/rrayYqZ4tf'>[Discord]</a>
                    </div>
                    <div class="spanner"></div>
                </div> -->

                <!-- <div class="paper" id="cvpr23"><img class="paper" src="images/cvpr23-rabit.png" title="rabit" />
                    <div> <strong>RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a Topological-consistent Dataset</strong><br />
                        <br>
                        Zhongjin Luo*, Shengcai Cai*, Jinguo Dong, Ruibo Ming, Liangdong Qiu, <strong>Xiaohang Zhan</strong>, Xiaoguang Han
                        <br>
                        <em>CVPR 2023</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2303.12564'>[Paper]</a>
                        <a href='https://gaplab.cuhk.edu.cn/projects/RaBit/'>[Project Page]</a>
                        <a href='https://github.com/zhongjinluo/RaBit'>[Code]</a>
                        <a href='https://gaplab.cuhk.edu.cn/projects/RaBit/dataset.html'>[Dataset]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="iclr23"><img class="paper" src="images/mfm.png" title="mfm" />
                    <div> <strong>Masked Frequency Modeling for Self-Supervised Visual Pre-Training</strong><br />
                        <br>
                        Jiahao Xie, Wei Li, <strong>Xiaohang Zhan</strong>, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                        <br>
                        <em>ICLR 2023</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2206.07706'>[Paper]</a>
                        <a href='https://www.mmlab-ntu.com/project/mfm/'>[Project Page]</a>
                        <a href='https://github.com/Jiahao000/MFM'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

            <!-- <div> <strong>2022</strong><br /> -->

                <!-- <div class="paper" id="pami"><img class="paper" src="images/pami-interclr.png" title="interclr" />
                    <div> <strong>Delving into Inter-Image Invariance for Unsupervised Visual Representations</strong><br />
                        <br>
                        Jiahao Xie, <strong>Xiaohang Zhan</strong>, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                        <br>
                        <em>IJCV 2022</em><br />
                        <br>
                        <a href='https://link.springer.com/article/10.1007/s11263-022-01681-x'>[Paper]</a>
                        <a href='https://github.com/open-mmlab/mmselfsup'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="pami"><img class="paper" src="images/pami-oltr.png" title="oltr" />
                    <div> <strong>Open Long-Tailed Recognition in a Dynamic World</strong><br />
                        <br>
                        Ziwei Liu*, Zhongqi Miao*, <strong>Xiaohang Zhan</strong>, Jiayun Wang, Boqing Gong, Stella X. Yu
                        <br>
                        <em>TPAMI 2022</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2208.08349'>[Paper]</a>
                        <a href='https://liuziwei7.github.io/projects/LongTail.html'>[Project Page]</a>
                        <a href='https://github.com/zhmiao/OpenLongTailRecognition-OLTR'>[Code]</a>
                        <a href='https://www.youtube.com/watch?v=A45wrs1g8VA'>[Demo]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="cvpr22"><img class="paper" src="images/cvpr22-styleerd.png" title="styleerd" />
                    <div> <strong>Style‐ERD: Responsive and Coherent Online Motion Style Transfer</strong><br />
                        <br>
                        Tianxin Tao, <strong>Xiaohang Zhan</strong>, Zhongquan Chen, Michiel van de Panne
                        <br>
                        <em>CVPR 2022</em><br />
                        <br>
                        <a href='https://openaccess.thecvf.com/content/CVPR2022/supplemental/Tao_Style-ERD_Responsive_and_CVPR_2022_supplemental.pdf'>[Paper]</a>
                        <a href='https://tianxintao.github.io/Online-Motion-Style-Transfer/'>[Project Page]</a>
                        <a href='https://github.com/tianxintao/Online-Motion-Style-Transfer'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="arxiv"><img class="paper" src="images/bsim.png" title="bsim" />
                    <div> <strong>Beyond Single Instance Multi-view Unsupervised Representation Learning</strong><br />
                        <br>
                        Xiangxiang Chu, <strong>Xiaohang Zhan</strong>, Xiaolin Wei
                        <br>
                        <em>BMVC 2022</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2011.13356'>[Paper]</a>
                    </div>
                    <div class="spanner"></div>
                </div>
                 -->
            <!-- <div> <strong>2021</strong><br /> -->

                


                <!-- <div class="paper" id="neurips"><img class="paper" src="images/neurips21-orl.png" title="orl" />
                    <div> <strong>Unsupervised Object-Level Representation Learning from Scene Images</strong><br />
                        <br>
                        Jiahao Xie, <strong>Xiaohang Zhan</strong>, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                        <br>
                        <em>NeurIPS 2021</em><br />
                        <br>
                        <a href='https://proceedings.neurips.cc/paper/2021/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Paper.pdf'>[Paper]</a>
                        <a href='https://www.mmlab-ntu.com/project/orl/'>[Project Page]</a>
                        <a href='https://github.com/Jiahao000/ORL'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="iccv21"><img class="paper" src="images/iccv21-detco.png" title="detco" />
                    <div> <strong>DetCo: Unsupervised Contrastive Learning for Object Detection</strong><br />
                        <br>
                        Enze Xie*, Jian Ding*, Wenhai Wang, <strong>Xiaohang Zhan</strong>, Hang Xu, Zhenguo Li, Ping Luo
                        <br>
                        <em>ICCV 2021</em><br />
                        <br>
                        <a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_DetCo_Unsupervised_Contrastive_Learning_for_Object_Detection_ICCV_2021_paper.pdf'>[Paper]</a>
                        <a href="https://github.com/xieenze/DetCo">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

            <!-- <div> <strong>2020</strong><br /> -->
                
<!-- 
                

                <div class="paper" id="CVPR20"><img class="paper" src="images/cvpr20-clustering.png" title="cvpr20-clustering" />
                    <div> <strong>Learning to Cluster Faces via Confidence and Connectivity Estimation</strong><br />
                        <br>
                        Lei Yang, Dapeng Chen, <strong>Xiaohang Zhan</strong>, Rui Zhao, Chen Change Loy, Dahua Lin
                        <br>
                        <em>CVPR 2020</em><br />
                        <br>
                        <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_to_Cluster_Faces_via_Confidence_and_Connectivity_Estimation_CVPR_2020_paper.pdf'>[Paper]</a>
                        <a href='http://yanglei.me/project/ltc_v2/'>[Project Page]</a>
                        <a href="https://github.com/yl-1993/learn-to-cluster">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR20"><img class="paper" src="images/cvpr20-ocda.png" title="cvpr20-ocda" />
                    <div> <strong>Compound Domain Adaptation in an Open World</strong><br />
                        <br>
                        Ziwei Liu*, Zhongqi Miao*, Xingang Pan, <strong>Xiaohang Zhan</strong>, Stella X. Yu, Dahua Lin, Boqing Gong
                        <br>
                        <em>CVPR 2020 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Open_Compound_Domain_Adaptation_CVPR_2020_paper.pdf'>[Paper]</a>
                        <a href='https://liuziwei7.github.io/projects/CompoundDomain.html'>[Project Page]</a>
                        <a href='https://www.youtube.com/watch?v=YcmgCCRA1qc'>[Demo]</a>
                        <a href='https://github.com/zhmiao/OpenCompoundDomainAdaptation-OCDA'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <!-- <div> <strong>2019</strong><br /> -->

                <!-- <div class="paper" id="ICCV2019"><img class="paper" src="images/iccv19w-codc.png" title="iccv19w-codc" />
                    <div> <strong>Collaborative Online Deep Clustering for Unsupervised Representation Learning</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Jiahao Xie, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                        <br>
                        <em>The <font color="orange">Champion</font> of <a href="https://sites.google.com/view/fb-ssl-challenge-iccv19/home">Facebook AI Self-Supervision Challenge</a> (all tracks), in ICCV <a href="https://sites.google.com/view/extremevision/">Extreme Vision Workshop</a>, 2019</em><br />
                        <br>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="ICCV2019"><img class="paper" src="images/iccv19-sw.png" title="iccv19-sw" />
                    <div> <strong>Switchable Whitening for Deep Representation Learning</strong><br />
                        <br>
                        Xingang Pan, <strong>Xiaohang Zhan</strong>, Jianping Shi, Xiaoou Tang, Ping Luo
                        <br>
                        <em>ICCV 2019</em><br />
                        <br>
                        <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Switchable_Whitening_for_Deep_Representation_Learning_ICCV_2019_paper.pdf">[Paper]</a>
                        <a href="https://github.com/XingangPan/Switchable-Whitening">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>


                
            </div> -->

            <!-- <div> <strong>2018</strong><br /> -->
                

                <!-- <div class="paper" id="AAAI2018"><img class="paper" src="images/aaai18-mm.png" title="aaai18-mm" />
                    <div> <strong>Mix-and-Match Tuning for Self-Supervised Semantic Segmentation</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Ziwei Liu, Ping Luo, Xiaoou Tang, Chen Change Loy
                        <br>
                        <em>AAAI 2018 (<font color="orange"><strong>Spotlight</strong></font>)</em><br />
                        <br>
                        <a href='https://ojs.aaai.org/index.php/AAAI/article/view/12331'>[Paper]</a>
                        <a href='http://mmlab.ie.cuhk.edu.hk/projects/M&M/'>[Project Page]</a>
                        <a href='https://github.com/XiaohangZhan/mix-and-match'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

            </div>
        </div>
    </div> -->


    <!-- <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Other Projects</h2>
            <div class="paper"><img class="paper" src="images/CULane.png" title="culane" />
                <div> <strong>CULane Dataset</strong><br />
                    <br>
                    CULane is a large scale challenging dataset for academic research on traffic lane detection.
                    <br>
                    <a href='https://xingangpan.github.io/projects/CULane.html'>[Project Page]</a>
                </div>
                <div class="spanner"></div>
            </div>
        </div>
    </div> -->

    <!---
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Contests</h2>
            <div class="paper">
                <ul>

                </ul>
                <div class="spanner"></div>
            </div>
        </div>
    </div>
    -->

    <!-- <div style="clear: both;">
        <div class="section"><h2>Invited Talks</h2>
            <div class="paper">
                <li>2017 @ JD.com: <a href=pdf/talk-jd-2017.pdf>Mix-and-Match Tuning for Self-Supervised Semantic Segmentation</a></li>
                <li>2019 @ HKSTP: <a href=pdf/talk-hkstp-2019.pdf>Self-Supervised Learning via Conditional Motion Propagation</a></li>
                <li>2020 @ SenseTime TITAN Open Classes <a href='https://www.bilibili.com/video/av883608955/'>[Video Record]</a>: <a href=pdf/talk-st-2020.pdf>Self-Supervised Learning</a></li>
                <li>2020 @ Tsinghua: <a href=pdf/talk-thu-2020.pdf>Exploiting Unlabeled Data in Computer Vision</a></li>
                <li>2020 @ Zhidongxi Open Classes <a href='https://course.zhidx.com/c/Yjk1ZjYxMmI2NjIyNTg1YmY2MDM='>[Video Record]</a>: <a href=pdf/talk-zhidongxi-2020.pdf>Understanding and Practice of OpenSelfSup: an Open-source Library for Self-Supervised Learning</a></li>
                <li>2021 @ TechBeat <a href='https://www.techbeat.net/talk-info?id=558'>[Video Record]</a>: Occlusion Problems in Natural Scenes</li>
                <li>2022 @ Huawei Developer Conference <a href='https://developer.huawei.com/consumer/en/training/course/live/C101667029313149892'>[Video Record]</a>: 3D Creation Pipeline Upgraded, Anything Can Be Animated</li>
            </div>
            <div class="spanner"></div>
        </div>

    </div> -->


    <div style="clear: both;">
        <div class="section"><h2>Honors and Awards</h2>
            <div class="paper">
                <li>Winner Future of <a href='https://www.kaggle.com/competitions/um-game-playing-strength-of-mcts-variants'>UM - Game-Playing Strength of MCTS Variants</a>, Maastricht University</li>
                <li>Gold Medal (2017), Silver Medal (2018) of Physics, Olympic 10/3, Nguyen Du High School for the Gifted, Dak Lak</li>
                <!-- <li>Top 100 Chinese Rising Stars in Artificial Intelligence, <a href="https://xueshu.baidu.com/usercenter/index/aischolar">AI华人新星百强</a> (100 out of 45205), 2021, Baidu Scholar.</li>
                <li>Huawei Genius Youth Program (华为天才少年计划), 2020</li>
                <li>Winner of <a href='https://sites.google.com/view/fb-ssl-challenge-iccv19/home'>Facebook AI Self-Supervision Challenge 2019</a></li>
                <li>Hong Kong PhD Fellowship, 2017, the Research Grants Council (RGC) of Hong Kong</li>
                <li>Outstanding Undergraduates Award of Tsinghua University, 2016 (63 winners out of about 3500 graduates)</li>
                <li>Outstanding Undergraduates Award of Beijing, 2016</li>
                <li>National Scholarship, 2014, Ministry of Education of China</li>
                <li>Technological Innovation Scholarship, 2014, 2015, Tsinghua University</li>
                <li>First class Academic Scholarship, 2013, 2014, Tsinghua University</li>
                <li>FastGear Special class Integrated Excellence Scholarship, 2013, Tsinghua University</li>
                <li>“Spark” Innovative Talent Cultivation Program for Students of Tsinghua University (50 out of about 3500)</li> -->
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section"><h2>Certifications</h2>
            <div class="paper">
               <li>DeepLearning.AI TensorFlow Developer Professional Certificate</li>
               <li>DeepLearning.AI Data Engineering Professional Certificate</li>
               <li>Generative AI for Software Development Skill Certificate</li>
               <li>IBM Data Science Professional Certificate</li>
               <li>IBM Data Analyst Professional Certificate</li>
               <li>IBM Data Engineering Professional Certificate</li>
               <li>IBM AI Developer Professional Certificate</li>
               <li>IBM AI Product Manager Professional Certificate</li>
               <li>IBM IT Project Manager Professional Certificate</li>
               <li>IBM AI Engineering Professional Certificate</li>
               <li>IBM IT Project Manager Professional Certificate</li>
               <li>IBM Machine Learning Professional Certificate</li>
               <li>Google Data Analytics Professional Certificate</li>
               <li>Google Advanced Data Analytics Professional Certificate</li>
               <li>Google Project Management: Professional Certificate</li>
               <li>Google IT Automation with Python Professional Certificate</li>
               <li>Preparing for Google Cloud Certification: Cloud Engineer Professional Certificate</li>
               <li>Google Cloud Data Analytics Professional Certificate</li>
               <li>Preparing for Google Cloud Certification: Cloud Data Engineer Professional Certificate</li>
               <li>Microsoft Azure Data Scientist Associate (DP-100) Exam Prep Professional Certificate Professional Certificate</li>
               <li>Microsoft Azure AI Fundamentals AI-900 Exam Prep Specialization</li>
               <!-- <li></li>
               <li></li>
               <li></li>
               <li></li>
               <li></li> -->

            </div>
        </div>
    </div>
    <div style="clear: both;">
        <div class="section"><h2>Blog</h2>
            <div class="paper">
                
               <li>You can visit my <a href="https://visionblog.github.io" target="_blank">Vision Blog</a> to see more details about my talks. Thank you for your interest!</li>
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section"><h2>My Knowledge</h2>
            <div class="paper">
               <li>For a comprehensive list of every term, word, and knowledge I have encountered and solved in my life, please visit my <a href="otherpage/kaneknowledge.html" target="_blank">Kane Knowledge</a> page.</li>
            </div>
        </div>
    </div>



    <div style="clear:both;">
    <p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>
<hr>
<!-- <div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=265499&w=400&t=tt&d=mEeGHmaub5GXeeTxKsShipAjJG4IG43WXN86qcXppc4&co=eeeeee&ct=ffffff&cmo=e03a3a&cmn=46cc3a"></script> -->
<!-- <div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=gesMVX0afDpGO4nqwqjTOhpvJ0_spfHjnCtB9Q2B0ns&cl=ffffff&w=a"></script> -->

<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=gesMVX0afDpGO4nqwqjTOhpvJ0_spfHjnCtB9Q2B0ns&cl=ffffff&w=a"></script> -->


<!-- <a href="https://clustrmaps.com/site/1c1kd"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=gesMVX0afDpGO4nqwqjTOhpvJ0_spfHjnCtB9Q2B0ns&cl=ffffff" /></a> -->


<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=gesMVX0afDpGO4nqwqjTOhpvJ0_spfHjnCtB9Q2B0ns&cl=ffffff&w=a"></script>
<!-- <a href="https://clustrmaps.com/site/1c1kd"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=gesMVX0afDpGO4nqwqjTOhpvJ0_spfHjnCtB9Q2B0ns&cl=ffffff" /></a> -->
</body>
    </html>
